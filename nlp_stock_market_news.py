# -*- coding: utf-8 -*-
"""NLP-Stock_market_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qX7PGZqb2YYz1B5qakBJrUKlkFqOX1yM
"""

pip install finnhub-python

# Import required libraries
import finnhub
import pandas as pd
from textblob import TextBlob
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd
import seaborn as sns
import spacy
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_error

#Setup Finnhub client
finnhub_client = finnhub.Client(api_key='ctap869r01qgsps7lk20ctap869r01qgsps7lk2g')

#Fetch company news for AAPL
res = finnhub_client.company_news('AAPL', _from="2024-01-01", to="2024-01-31")

# Convert the result into a DataFrame
news_data = pd.DataFrame(res)

news_data.info()

"""# 1. Sentiment Analysis"""

# Check if the data has content
if news_data.empty:
    print("No news data fetched. Please check your API key or date range.")
else:
    # Perform sentiment analysis on 'headline' and 'summary'
    news_data['headline_sentiment'] = news_data['headline'].apply(lambda x: TextBlob(x).sentiment.polarity if pd.notnull(x) else None)
    news_data['summary_sentiment'] = news_data['summary'].apply(lambda x: TextBlob(x).sentiment.polarity if pd.notnull(x) else None)

news_data.info()

# Display the first few rows of the DataFrame
print(news_data.head())

# Plot sentiment distribution for headlines and summaries
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(news_data['headline_sentiment'], bins=20, kde=True, color='blue')
plt.title('Headline Sentiment Distribution')
plt.xlabel('Sentiment Polarity')

plt.subplot(1, 2, 2)
sns.histplot(news_data['summary_sentiment'], bins=20, kde=True, color='green')
plt.title('Summary Sentiment Distribution')
plt.xlabel('Sentiment Polarity')

plt.tight_layout()
plt.show()

"""# 2. Impact of Sentiment on Stock Market Price"""

# Assume you already have stock data merged with sentiment data
stock_data = pd.DataFrame({
    'date': pd.date_range(start="2024-01-01", periods=226),
    'stock_price': np.random.uniform(150, 200, 226)
})

# Assume 'news_data' is available (replace this with actual news data)
news_data = {
    'datetime': pd.date_range(start="2024-01-01", periods=226),
    'headline_sentiment': np.random.uniform(-1, 1, 226),  # Random sentiment values
    'summary_sentiment': np.random.uniform(-1, 1, 226)   # Random sentiment values
}
news_data_df = pd.DataFrame(news_data)

# Function to calculate correlation and plot scatterplots
def analyze_sentiment_vs_stock(sentiment_column):
    if sentiment_column in news_data_df.columns:
        # Merge stock and sentiment data
        merged_data = pd.concat([news_data_df[['datetime', sentiment_column]], stock_data], axis=1)
        merged_data.dropna(subset=[sentiment_column, 'stock_price'], inplace=True)  # Remove rows with missing values

        # Calculate correlation
        correlation = merged_data[sentiment_column].corr(merged_data['stock_price'])
        print(f"Correlation between {sentiment_column} and stock price: {correlation:.2f}")
                # Plot scatterplot
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=merged_data, x=sentiment_column, y='stock_price', color='purple')
        plt.title(f'{sentiment_column.capitalize()} vs Stock Price')
        plt.xlabel(sentiment_column.replace("_", " ").capitalize())
        plt.ylabel('Stock Price')
        plt.show()
    else:
        print(f"Error: {sentiment_column} column not found in news data!")

# Analyze headline sentiment vs stock price
analyze_sentiment_vs_stock('headline_sentiment')

# Analyze summary sentiment vs stock price
analyze_sentiment_vs_stock('summary_sentiment')

"""# Sentiment Analysis on Stock News"""

# Fetch news articles for Apple (AAPL)
try:
    news_data = finnhub_client.company_news('AAPL', _from="2024-01-01", to="2024-01-31")
    news_df = pd.DataFrame(news_data)

    # Apply sentiment analysis on the headline
    news_df['headline_sentiment'] = news_df['headline'].apply(lambda x: TextBlob(x).sentiment.polarity)

    # Check the sentiment of the first few articles
    print(news_df[['headline', 'headline_sentiment']].head())

except finnhub.FinnhubAPIException as e:
    print(f"Error: {e}")

# Fetch news articles for Apple (AAPL)
try:
    news_data = finnhub_client.company_news('AAPL', _from="2024-01-01", to="2024-01-31")
    news_df = pd.DataFrame(news_data)

    # Apply sentiment analysis on the headline
    news_df['summary_sentiment'] = news_df['headline'].apply(lambda x: TextBlob(x).sentiment.polarity)

    # Check the sentiment of the first few articles
    print(news_df[['headline', 'summary_sentiment']].head())

except finnhub.FinnhubAPIException as e:
    print(f"Error: {e}")

"""#Training"""

# We will use the 'headline_sentiment' and 'summary_sentiment' as features to predict 'stock_price'
merged_data = pd.concat([news_data_df[['datetime', 'headline_sentiment', 'summary_sentiment']], stock_data], axis=1)
merged_data.dropna(subset=['headline_sentiment', 'summary_sentiment', 'stock_price'], inplace=True)

# Split the data into features (X) and target variable (y)
X = merged_data[['headline_sentiment', 'summary_sentiment']]  # Features
y = merged_data['stock_price']  # Target variable (stock price)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# Define your model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror')

param_grid = {
    'n_estimators': [100, 200],              # Number of boosting rounds
    'learning_rate': [0.01, 0.1, 0.2],       # Step size
    'max_depth': [3, 5],                      # Max depth of trees
    'min_child_weight': [1, 5],                # Minimum child weight
    'subsample': [0.8, 1.0],                   # Subsample ratio
    'colsample_bytree': [0.8, 1.0],            # Column sample for each tree
    'gamma': [0, 0.1],                        # Minimum loss reduction
    'lambda': [1],                             # L2 regularization
    'alpha': [0],                              # L1 regularization
}

# Example for grid search with parallelization
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=2,
                           n_jobs=-1, verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Best hyperparameters and best score
print(f"Best Hyperparameters: {grid_search.best_params_}")
print(f"Best RMSE: {np.sqrt(-grid_search.best_score_)}")

# Predict on test data
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate RMSE and R-squared
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Test RMSE: {rmse}")
print(f"Test R-squared: {r2}")

"""# Named Entity Recognition (NER) on News Data"""

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Perform NER on headlines
def extract_entities(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Convert news_data to a DataFrame if it's a list
if isinstance(news_data, list):
    news_data = pd.DataFrame(news_data)

news_data['entities'] = news_data['headline'].apply(extract_entities)

# Display extracted entities
print(news_data[['headline', 'entities']].head())

# Count entity types
all_entities = [ent[1] for ents in news_data['entities'] for ent in ents]
entity_counts = pd.Series(all_entities).value_counts()

# Plot entity counts
plt.figure(figsize=(10, 6))
entity_counts.plot(kind='bar', color='skyblue')
plt.title('Entity Type Distribution')
plt.xlabel('Entity Type')
plt.ylabel('Count')
plt.show()